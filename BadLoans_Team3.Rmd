---
title: "BadLoans"
author: "[redacted]"
date: "11/12/2022"
output:
  html_document: default
  pdf_document: default
---

## R Markdown
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

**This exercise is under construction. Please report any errors at https://forms.gle/2W4tffs4YJA1jeBv9**

Goal: 
Understand and experience logistic regression to predict the probability of loan default (due to fraud or other reasons).
Build skills and confidence to search for online help.

Background:
The data for this question contains information about borrowers, loans, and the outcome (defaulted or paid). We are concerned about minimizing loss, and not concerned whether the default was intentiona/fraud or unintentional. I developed this assignment to walk you through the process because I couldn't find any assignment at this level that can balance fundamentals and practical aspects. The data has been adapted from https://campus.datacamp.com/courses/credit-risk-modeling-in-r/ (but my approach is quite different).


Before starting:
1. You are not allowed to:
   1a. Search for solutions to this assignment
   2b. Subcontract your assignment to someone else
2. You are allowed to:
   2a. Search information about packages and functions you may use
   2b. Consult with your team mates.


Individual assignment only: 175 total points (Rmd and html solution)

## [1 point] Q1.
Start by entering your name and today's date in Lines 3 and 4, respectively, to indicate your compliance with the Fuqua Honor Code.
Then, run the chunk of code below by clicking on the green arrow (that points to the right) on the top right of the chunk.
*Tip:* I numbered code chunks corresponding to their numbers. Chunk 1 specifies the knitting parameters.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## [6 points] Q2.
Read and store the data from the file *LoanData.rds* into a variable called *loanData*.
Then, inspect the data using 2 or more R commands.
*Tip:* Use Google to learn about rds file format and how to read it into R. You'll likely need some libraries and packages.
*Rubric:* 3 each point for reading, 1 point for storing; 1 points each for using 2 R commands for inspecting.*

```{r}
#rm(list = ls())
ls()

#install.packages("tidyverse") 
#install.packages("dplyr") 

library(dplyr)
library(readr)
loanData  = read_rds("LoanData.rds")

# Check the loanData data frame
glimpse(loanData)
str(loanData)
summary(loanData)



```

## [4 points] Q3.
I haven't provided you the data dictionary but the columns have descriptive names so you can easily interpret them. Perhaps, *homeLiving* and *creditGrade* are unclear. Figure what *homeLiving* and *creditGrade* represent by examining the the summary.
*Rubric:* 2 points for each (*homeLiving* and *creditGrade*).

```{r}
### This section doesn't require code. Just answer below (outside) the code block.
# Begin Answer
# 1. The *homeLiving* column indicates whether borrower rents the home or owns it with or without a mortgage.
# 2. The *creditGrade* column is the home owner's credit rating ranging from A to G.
# End Answer
```

## [6 points] Q4.
Now, let's make sure we understand credit grade using crosstables *isLoanDefault* and *creditGrade*.
*Tip:* Follow these steps to make progress... 
1. Find, install, and load the library with CrossTable function.
2. Call CrossTable() on *creditGrade* and *isLoanDefault* columns of loanData.
*Rubric:* 4 points for finding, installing and loading the correct libraries; 2 points for calling CrossTable with the correct parameters.

```{r}
# https://community.rstudio.com/t/cant-find-function-crosstable/91752

#install.packages("gmodels")
library(gmodels)
CrossTable(loanData$creditGrade, loanData$isLoanDefault)

```

## [4 points] Q5.
Based on the above answers, deduce whether or not A is the best (highest) credit rating. Explain your reasoning. 
*Tip:* The third number in each box under the loanData$isLoanDefault column is the proportion of default rate. For example, the default rate for A was 0.059 (i.e., 5.9%). You do not need to understand CrossTable beyond that to answer this question.
*Rubric:* 1 point for the correct answer, 3 points for the reasoning.

```{r}
### This section doesn't require code. Just answer below (outside) the code block.
# Begin Answer
# A is the best credit rating because it has the lowest default rate (0.059) on loans comparing to others. 
# End Answer
```

## [12 points] Q6.
Let's check for any outliers in the numeric/integer columns related to the borrowers (employmentYears, incomeAnnual, ageYears) in steps:
1. Examine the summary statistics to pick large relative gaps between the minimum and the 1st quartile as well as the maximum and the 3rd quartile. 
2. Then, display a scatter plot of all combinations of two variables, one at a time.
*Tip:* I prefer to visualize outliers using scatter plots as a combination of two variables, so I can get a better understanding of the outliers, such as a huge salary for a young person (in age or employment duration).
*Tip:* library(ggplot2) and library(gridExtra) may be useful.
*Tip:* The syntax of each plot will look something like this plotName = ggplot(loanData, aes(y = ... , x = ... , color = isLoanDefault)) + geom_point() + geom_smooth(method = "lm") + labs(x = "...", y = "...", title = "...")
*Tip:* You can see them in a one plot using grid.arrange(pIncomeAge, pIncomeEmployment, pEmploymentAge, nrow = ...)
*Rubric:* 3 points for each plot; 3 point for examining the summary data (on honor code).

```{r}
library(ggplot2)
library(gridExtra)
library(dplyr)

pIncomeAge = ggplot(loanData, aes(y = incomeAnnual, x = ageYears, color = isLoanDefault)) + geom_point() + geom_smooth(method = "lm") + labs(x = "Age (Years)", y = "Income (Year)", title = "Income vs Age")
pIncomeAge 

pIncomeEmployment = ggplot(loanData, aes(y = incomeAnnual, x = employmentYears, color = isLoanDefault)) + geom_point() + geom_smooth(method = "lm") + labs(x = "Employment (Years)", y = "Income (Year)", title = "Income vs Employment")
pIncomeEmployment 

pEmploymentAge = ggplot(loanData, aes(y = employmentYears, x = ageYears, color = isLoanDefault)) + geom_point() + geom_smooth(method = "lm") + labs(x = "Employment (Years)", y = "Income (Year)", title = "Emplpoyment vs Age")
pEmploymentAge

grid.arrange(pIncomeAge, pIncomeEmployment, pEmploymentAge, nrow = 2)

```

## [6 points] Q7.
Now, identify and remove any outliers and store the result in *loanDataNoOutliers*.
Then, examine loanDataNoOutliers to verify that your code worked.
*Tip:* I only removed one row. The others may be useful for our model. 
*Tip:* The structure of *loanData* and *loanDataNoOutliers* should be identical expect the number of rows be different (because you removed one or more rows).
*Rubric:* 1 point for each column; 2 point for removing, 1 point for checking.

```{r}
# Note that there is no NA for incomeAnnual and ageYears by checking sum(is.na(loanData$incomeAnnual)) and sum(is.na(loanData$ageYears))
# There is NAs for employmentYears by checking sum(is.na(loanData$employmentYears))
# sum(is.na(loanData$incomeAnnual))
# sum(is.na(loanData$ageYears))
# sum(is.na(loanData$employmentYears))

loanDataNoOutliers = loanData %>%
                      filter((ageYears > 18) & (ageYears < 90)) %>%
                      filter((incomeAnnual > 0) & (incomeAnnual < 300000)) %>%
                      filter(((employmentYears < 60) & (employmentYears >= 0)) | (is.na(employmentYears)))

str(loanDataNoOutliers)

```

## [3 points] Q8.
Should you report any removed row(s) for further investigation?

```{r}
### This section doesn't require code. Just answer below (outside) the code block.
# Begin Answer
# No need to report any removed row(s) for future investigation because they are outliers.
# End Answer
```

## [4 points] Q9.
Why do we store the the loanData in a new variable loanDataNoOutliers? Can you imagine a scenario when you would be better off not storing the result?
*Rubric:* 2 points for each answer.

```{r}
### This section doesn't require code. Just answer below (outside) the code block.
# Begin Answer
# 1. We should maintain a copy of the original data and/or any intermediate data so we can retrace our steps in case we want to undo something.
# 2. We should consider not storing the original and/or intermediate data if the data set is so large that storing can be computationally expensive (space and time). Also, sometimes we won't store each step if there are too many intermediate steps for us (and perhaps we save a new copy only when we make major changes).
# End Answer
```

## [9 points] Q10.
Now, let's handle missing data by:
1. Make a copy of *loanDataNoOutliers* and call it *loanDataNoOutliersNA*. Then, conduct the following operations on *loanDataNoOutliersNA*.
2. Compute the percentage missing (NA) values in each column. 
3. Add a new column for each variable that has missing values, as missing data can be an fraud indicator. The value of this column is 1 to indicate rows with missing values, and 0 otherwise.
4. Replace the missing values with the median value of the column if there are fewer than 10% missing values.
5. Delete the entire column if there are 10% or more missing values.
*Tip:* mean(is.na(dfName$colName))*100 gives you the percentage of missing values in colName of dfName.
*Tip:* median(dfName$colName, na.rm = TRUE) gives you the median value (ignoring all NAs).
*Tip:* The summary should be similar except the added columns and imputed data.
*Tip:* In real life, I would have asked you to use better imputation methods than just replacing by median.
*Rubric:* 0.5 points for checking each column; 4 point for handling NAs, 1 point for checking.

```{r}
# Print the column names str(loanDataNoOutliersNA)
#'data.frame':	24457 obs. of  8 variables:
# $ isLoanDefault  : int  0 0 0 0 0 0 0 1 0 0 ...
# $ loanAmount     : int  5000 2400 10000 5000 3000 12000 3000 10000 10000 3600 ...
# $ interestRate   : num  10.6 NA 13.5 NA NA ...
# $ creditGrade    : Factor w/ 7 levels "A","B","C","D",..: 2 3 3 1 5 2 2 2 3 1 ...
# $ employmentYears: int  10 25 13 3 9 11 3 3 4 13 ...
# $ homeLiving     : Factor w/ 4 levels "MORTGAGE","OTHER",..: 4 4 4 4 4 3 4 4 4 1 ...
# $ incomeAnnual   : num  24000 12252 49200 36000 48000 ...
# $ ageYears       : int  33 31 24 39 24 28 22 28 23 27 ...

# Using print(c(x,y..)) to get which columns have the fewer than 10% missing values

print(c("The percentage of missing (NA) values for isLoanDefault is: ",  round(mean(is.na(loanDataNoOutliers$isLoanDefault))*100, 2)))
print(c("The percentage of missing (NA) values for loanAmount is: ",  round(mean(is.na(loanDataNoOutliers$loanAmount))*100, 2)))
print(c("The percentage of missing (NA) values for interestRate is: ",  round(mean(is.na(loanDataNoOutliers$interestRate))*100, 2)))
print(c("The percentage of missing (NA) values for creditGrade is: ",  round(mean(is.na(loanDataNoOutliers$creditGrade))*100, 2)))
print(c("The percentage of missing (NA) values for employmentYears is: ",  round(mean(is.na(loanDataNoOutliers$employmentYears))*100, 2)))
print(c("The percentage of missing (NA) values for homeLiving is: ",  round(mean(is.na(loanDataNoOutliers$homeLiving))*100, 2)))
print(c("The percentage of missing (NA) values for incomeAnnual is: ",  round(mean(is.na(loanDataNoOutliers$incomeAnnual))*100, 2)))
print(c("The percentage of missing (NA) values for ageYears is: ",  round(mean(is.na(loanDataNoOutliers$ageYears))*100, 2)))

# From above lines of codes, interestRate and employmentYears have the fewer than 10% missing values.
interestRate_na_true = is.na(loanDataNoOutliers$interestRate)
sum(interestRate_na_true)

employmentYears_na_true = is.na(loanDataNoOutliers$employmentYears)
sum(employmentYears_na_true)

# Make a copy of *loanDataNoOutliers* and call it *loanDataNoOutliersNA*
loanDataNoOutliersNA = loanDataNoOutliers

# Using median(dfName$colName, na.rm = TRUE) to get the median value for interestRate and assign it to those with NA value.
loanDataNoOutliersNA$interestRate[interestRate_na_true] = median(loanDataNoOutliersNA$interestRate, na.rm = TRUE) 

# Convert median value to integer and assign it to a new data frame named interestRate_NA
interestRate_NA = as.integer(interestRate_na_true)

# Combine loanDataNoOutliersNA with the new data frame interestRate_NA
loanDataNoOutliersNA = cbind(loanDataNoOutliersNA, interestRate_NA)

# Using median(dfName$colName, na.rm = TRUE) to get the median value for employmentYears and assign it to those with NA value.
loanDataNoOutliersNA$employmentYears[employmentYears_na_true] = median(loanDataNoOutliersNA$employmentYears, na.rm = TRUE) 

# Convert median value to integer and assign it to a new data frame named employmentYears_NA
employmentYears_NA = as.integer(employmentYears_na_true)

# Combine loanDataNoOutliersNA with the new data frame employmentYears_NA
loanDataNoOutliersNA = cbind(loanDataNoOutliersNA, employmentYears_NA)

```

## [9 points] Q11.
We should consider some of our features (aka variables and columns) for categorization because they don't have a clear relationship with the target variable. For example, a higher interest rate may cause higher default due to financial burden while a lower interest rate may cause a higher default due to complacency (or procrastination). Which other variables should you consider for categorization?
*Rubric:* 1 points for each feature (*isLoanDefault* is not a feature)

```{r}
### This section doesn't require code. Just answer below (outside) the code block.
# Begin Answer
# We will also consider annual income. High annual income usually will have lower default rate, but people with high annual income may borrow large amount of loans, which may cause them to default on their loans if they lose jobs.
# End Answer
```

## [10 points] Q12.
Before running regression, separate *loanDataNoOutliersNA* into training data (dataframe *loanTrain*) and test data (dataframe *loanTest*) using the sample() function. Start by *set.seed(2020)* so we can reproduce the randomization for multiple runs (see http://rfunction.com/archives/62 to learn more).
The dataframe *loanTest* should contains 1/3rds of *loanDataNoOutliersNA*'s rows. The dataframe *loanTrain* should contains the other 2/3rds of *loanDataNoOutliersNA*'s rows. 
Then, inspect the structure of *loanTrain* and *loanTest* to verify that your code works.
*Tip:* Follow the following steps:
1. set.seed(2020)
2. Use sample() to generate *loanTestIndices* as a sample of 1/3 of *loanDataNoOutliersNA*'s indices for testing. The remaining 2/3 will be used to train. (Note: Most people generate the 2/3 indices for training but the optimizer in me prefers to assign 1/3 as much work to my computer - this is a byproduct of working in real-time systems where every nanosecond counts).
3. Use these statements like the 2 statements below to split between test and training data.
*loanTest =  loanDataNoOutliersNA[loanTestIndices, ]* and *loanTrain = loanDataNoOutliersNA[-loanTestIndices, ]*
*Rubric:* 1 points for set.seed(), 3 points for correctly sampling, 4 points (2 each) for constructing loanTest and loanTrain, 2 point for verifying.

```{r}
set.seed(2020)

# Generate 1/3 data as the sample
loanTestIndices = sample(1:nrow(loanDataNoOutliersNA), nrow(loanDataNoOutliersNA)/3, replace = FALSE, prob = NULL)

# Split 1/3 sample from the data set and verify
loanTest =  loanDataNoOutliersNA[loanTestIndices, ]
str(loanTest)

# Split 2/3 sample from the data set and verify
loanTrain = loanDataNoOutliersNA[-loanTestIndices, ]
str(loanTrain)

```

## [6 points] Q13.
Run a logistic regression on *loanTrain* data and store the result in *glmBase*.
Then, print the summary of *glmBase*.
*Tip:* Your call will look something like the one below.
*glmBase = glm(target  ~ ., family = "binomial", data = trainingdata)*
Note: You will need to replace your call with the appropriate variables. (The . indicates that we will consider all features in the regression equation.)
*Tip:* I realize that some people would consider categorization at this stage. I like to learn more by running a regression now before considering categorizations. (Notice that we don't need to consider categorizing *naInterestRate* and *naEmploymentYears* because binary data is already a *factor* and ordering doesn't matter in binary data). 
*Rubric:* 5 points for the glm call, 1 point for summary

```{r}
# Running logistic regression on sample data loanTrain
glmBase = glm(isLoanDefault  ~ ., family = "binomial", data = loanTrain)
summary(glmBase)


```

## [9 points] Q14.
Examine the significance levels of your base model above by reviewing the *summary()*. Which variables are most significant? Which are mildly significant, and which are not very significant?
*Tip:* Focus on the last column (low *Pr(>|z|)* values) and asterisks (high significance) to determine which variables are most significant. The least significant variables have high *Pr(>|z|)* values and few asterisks. 
*Tip:* The first two columns are not useful because they are not scaled.
*Rubric:* 1 points for each feature

```{r}
### This section doesn't require code. Just answer below (outside) the code block.
# Begin Answer
# From the summary result, we can tell creditGrade (1.38e-08 - 9.04e-07), employmentYears_NA (8.49e-10) and incomeAnnual (< 2e-16) have the lowest *Pr(>|z|)* values, they are most significant.
# interestRate (0.0120) and employmentYears (0.0611) are medium important.
# loanAmount (0.7634), ageYears (0.3179), homeLiving (0.2665) are least important
# End Answer
```

## [10 points] Q15.
In real life, we would now try to optimize our model by:
  Filtering through hundreds of features (metaphorically)
  Consider categorizing or transforming variables with low significance
  etc.
However, for this exercise, we will start our predictions based on this model because we have only a few features. We may refine our model with transformations and categorizations later.
Specifically, carry out the following steps:
1. Compute the predictions for each row in the *loanTest* dataframe using the predict function and store the result in vector called *predictionsBase*. Modify the predict function below for your needs and/or Google for help.
   *predict(object = yourModel, newdata = test, type = "response")*
   where your object is your model, newdata is your test data, and type is always "response" (because that gives converts from log-odds to probability)
2. Then, append the *predictionsBase* vector to *loanTest* (as its last column).
3. Print a statistical summary of *predictionsBase*, *loanTest[, "isLoanDefault"]*, and *predictionsBase - loanTest[, "isLoanDefault"]* to check the accuracy the predictions. Relatively similar summary statistics and difference near zero and are good early signs.
*Rubric:* 6 points for correct use of predict, 1 point for appending the new column, 3 points (1 point each) for summary()

```{r}
# Running the predict function
predictionsBase = predict(object = glmBase, newdata = loanTest, type = "response")
summary(predictionsBase)

# Insert the predictionsBase vector to loanTest
loanTest = cbind(loanTest, predictionsBase)

# Print the statistical summary for predictionsBase, loanTest[, "isLoanDefault"] and predictionsBase - loanTest[, "isLoanDefault"]
summary(predictionsBase)

summary(loanTest[, "isLoanDefault"])

summary(predictionsBase - loanTest[, "isLoanDefault"])

```

## [4 points] Q16.
What are your thoughts on the summary of *predictionsBase - loanTest[, "isLoanDefault"]*?
*Tip:* Check the min, median, mean, max of the value.
*Rubric:* 1 points for observing the each item in the tip.

```{r}
### This section doesn't require code. Just answer below (outside) the code block.
# Begin Answer
# The min value of -0.979492
# The median value of 0.089643
# The mean value of -0.002525
# the max value of 0.527180
# End Answer
```

## [4 points] Q17.
Notice that *isLoanDefault* is 0 or 1, while *predictionsBase* is a probability value (that ranges from between 0 to about 0.5). That means we need to convert the probability distribution to a 0 or 1 prediction using a cutoff value that maps each probability to either 0 or 1. A natural cutoff value is 0.5. Count the number of rows that would be classified as 0 and 1, respectively, by using the cutoff of *0.5*.
*Tip:* Add up the number of TRUE values using sum on the respective logical conditions.
*Rubric:* 2 points each.

```{r}
# Below codes will also count the number of rows that have the probability value <= 0.5 or > 0.5 
# loanTest %>%
#  filter(predictionsBase <= 0.5) %>%
#  nrow()

# loanTest %>%
#  filter(predictionsBase > 0.5) %>%
#  nrow()

sum(loanTest[, "predictionsBase"] <= 0.5)
sum(loanTest[, "predictionsBase"] >  0.5)

```

## [6 points] Q18.
Consider the cutoff of 0.5 based on the output from the last chunk. Would you adjust the cutoff value? Why? How?
*Tip:* I learned this on my own and this was an important lesson for choosing the cutoff values.
*Rubric:* 2 points for each question above

```{r}
### This section doesn't require code. Just answer below (outside) the code block.
# Begin Answer
# Because there are only 3 loans with the probability value > 0.5, we will like to reduce thecutoof value to get the results more evenly distributed.
# End Answer


```

## [6 points] Q19. 
Now, set a value for a variable called *cutoff* that improves the cutoff value based on what you know about your model and the data. 
Then, use this *cutoff* value to compute a new column vector called *isPrediction* based on *predictionsBase*.
Then, add this vector as the first column of *loanTest* (so it can be side by side with *isLoanDefault*).
Finally, print a summary of *loanTest* to verify that your code works.
*Tip:* I based my *cutoff* on the fact that 0.1121 of test cases are bad loans. Specifically, I sorted the ** vector in non-decreasing order (using *sort()* function with *decreasing = TRUE*), and then I set the value of *cutoff* equal to value of the element number **as.integer(0.1121 * length(sortedPredictionBase)) of the sorted list.**
*Rubric:* 2 points each for choice of *cutoff* and generating is *isPrediction*; 1 point each for adding *isPrediction* as the first column of *loanTest* and verifying the result.

```{r}
# https://stackoverflow.com/questions/62437321/set-cutoff-threshold-when-predicting-in-r
# Sort the predictionsBase values
sortedPredictionBase = sort(predictionsBase, decreasing = TRUE)

# Get the ratio for default loans to decide the cutoff point, which is around the provided number 0.1121
#  loanTest %>%
#  filter(isLoanDefault == '1')
#  ratio = sum(loanTest$isLoanDefault) / nrow(loanTest)
#  ratio

cutoff = sortedPredictionBase[as.integer(0.1121 * length(sortedPredictionBase))]
print(c("So the cutoff value is ",  cutoff))

# Using below code will also work for the same purpose.
#loanTest = loanTest %>% 
#    mutate(isPrediction=ifelse(predictionsBase > ratio, 1, 0), .before = isLoanDefault)

isPrediction = as.integer(cutoff < predictionsBase)
isPrediction

# Add isPrediction as the first column of loanTest
loanTest = cbind(isPrediction, loanTest)

```

## [8 points] Q20. 
Now, compute the number of True Negatives, False Negatives, False Positives, True Positives, respectively. Store the results in tneg, fneg, fpos, and tpos, respectively.
Then, print all four values.
*Tip:* **tneg = sum( (0 == loanTest$isPrediction) & (0 == loanTest$isLoanDefault))**
*Rubric:* 2 point of each value.

```{r}


# Compute the number of True Negatives and print the value
tneg = sum(((0 == loanTest$isPrediction) & (0 == loanTest$isLoanDefault)))
tneg

# Compute the number of False Negatives and print the value
fneg = sum(((0 == loanTest$isPrediction) & (1 == loanTest$isLoanDefault)))
fneg

# Compute the number of False Positives and print the value
fpos = sum(((1 == loanTest$isPrediction) & (0 == loanTest$isLoanDefault)))
fpos

# Compute the number of True Positives and print the value
tpos = sum(((1 == loanTest$isPrediction) & (1 == loanTest$isLoanDefault)))
tpos

```

## [8 points] Q21. 
Now, let's use the *confusionMatrix()* function to gauge the model's confusion. The *confusionMatrix()* function is similar to our true/false negatives/positives calculations above but it also generates more data.
*Tip:* You will need to install/library caret and e1071 to call *confusionMatrix()* as shown below:
*confusionMatrix(data = as.factor(predicted), reference =as.factor(actual))*
Notice that we have to convert integers to factors. The predicted parameter is *loanTest$isPrediction* and the actual parameter is *loanTest$isLoanDefault*.
*Rubric:* 4 points for installing libraries. 4 points for using *confusionMatrix()*.

```{r}
#install.packages("caret")
#install.packages("e1071")

library(caret)
library(e1071)

# https://search.r-project.org/CRAN/refmans/caret/html/confusionMatrix.html
# https://rpubs.com/motox249/867662

levels(as.factor(loanTest$isPrediction))
levels(as.factor(loanTest$isLoanDefault))

confusionMatrix(data = as.factor(loanTest$isPrediction), reference =as.factor(loanTest$isLoanDefault))

```

## [6 points] Q22.
Now, let's return to our choice of cutoff to understand how this choice impacts the Accuracy, Sensitivity, and Specificity.
Run the code below to print the confusion matrix for *cutoff* you used earlier, and verify that this gives you the same results as the previous chunk (while bypassing a lot of of other code).
Then, learn about confusion matrix by Googling "accuracy sensitivity specificity confusion matrix" and/or reading https://towardsdatascience.com/taking-the-confusion-out-of-confusion-matrices-c1ce054b3d3e.
*Rubric:* 1 point for running the code; 3 points for learning more about confusion matrix (based on honor code)

```{r}
### Do not edit this code, just run it!
confusionMatrix(data = as.factor(as.numeric(cutoff < loanTest$predictionsBase)), 
                reference = as.factor(loanTest$isLoanDefault))
```

## [2 points] Q23.
Now, increase the cutoff by 10 percent in the code for confusion matrix above.
*Tip:* Ignore any warnings.

```{r}
confusionMatrix(data = as.factor(as.numeric(cutoff*(1 + 0.1) < loanTest$predictionsBase)), 
                reference = as.factor(loanTest$isLoanDefault))

```

## [3 points] Q24.
Review the results above and complete the phrased below by using the word increased or decreased to indicate if the following parameters increased or decreased compared to your original cutoff: *Accuracy*, *Sensitivity*, and *Specificity*?
*Rubric:* 1 point for each Accuracy, Sensitivity, and Specificity

```{r}
### Complete the following phrases:
## The Accuracy increased from 0.8248 to 0.8511.
## The Sensitivity increased from 0.9016 to 0.9403.
## The Specificity decreased from 0.2192 to 0.1483.
```

## [2 points] Q25.
Now, decrease the cutoff by 10 percent (from Q22) in the code for confusion matrix.
*Tip:* Ignore any warnings.

```{r}
confusionMatrix(data = as.factor(as.numeric(cutoff*(1 - 0.1) < loanTest$predictionsBase)), 
                reference = as.factor(loanTest$isLoanDefault))

```

## [3 points] Q26.
Review the results above and complete the phrased below by using the word increased or decreased to indicate if the following parameters increased or decreased compared to your original cutoff: *Accuracy*, *Sensitivity*, and *Specificity*?
*Rubric:* 1 point for each Accuracy, Sensitivity, and Specificity

```{r}
### Complete the following phrases:
## The Accuracy decreased from 0.8248 to 0.7832.
## The Sensitivity decreased from 0.9016 to 0.8434.
## The Specificity increased from 0.2192 to 0.3085.

```

## [5 points] Q27.
Find the maximum value you can set to replace 0.1 below without generating a warning.
*Tip:* The summary of predictionsBase will give you a clue.

```{r}
### Do not edit any other than 0.1 in this code, just run it!
summary(loanTest$predictionsBase)
confusionMatrix(data = as.factor(as.numeric(0.5271 < loanTest$predictionsBase)), 
                reference = as.factor(loanTest$isLoanDefault))

# By changing the value, we found that 0.5271 is the maximum number that will not generating a warning.
```

## [3 points] Q28.
What is the danger in maximizing the accuracy?
*Tip:* Review the confusion matrices above.

```{r}
### This section doesn't require code. Just answer below (outside) the code block.
# Begin Answer
# By maximizing accuracy, we will have more loans in default.
# End Answer
```

## [5 points] Q29.
Examine the output from Chunk 13 and run a new regression with (only) the top 5 predictors of the target variable, and store the result in *glmTop5*. 
Then, print the summary of *glmTop5*.
*Rubric:* 1 point for each of the top 5 predictors.

```{r}
# Copy the formula from Chunk 13 and apply the top 5 predictors from the summary result, they are creditGrade (1.38e-08 - 9.04e-07), employmentYears_NA (8.49e-10) and incomeAnnual (< 2e-16), these 3 predictors have the lowest *Pr(>|z|)* values. The next 2 predictors from the summary result are interestRate (0.0120) and employmentYears (0.0611).

glmTop5 = glm(isLoanDefault  ~ creditGrade + employmentYears_NA + incomeAnnual + interestRate + employmentYears, family = "binomial", data = loanTrain)
summary(glmTop5)


```

## [3 points] Q30.
Examine the output from the previous chunk and run a new regression with (only) the top 3 predictors of the target variable, and store the result in *glmTop3*. 
Then, print the summary of *glmTop3*.
*Rubric:* 1 point for each of the top 3 predictors.

```{r}
# Remove the two medium important variables from the formula in Chunk 29, which are interestRate (0.0120) and employmentYears (0.0611).
glmTop3 = glm(isLoanDefault  ~ creditGrade + employmentYears_NA + incomeAnnual, family = "binomial", data = loanTrain)
summary(glmTop3)


```

## [3 points] Q31.
Based on the AIC scores which one of the following would you choose for modeling and why: glmBase, glmTop5, glmTop3?

```{r}
### This section doesn't require code but feel free to reprint any critical values.
# Begin Answer
# https://www.scribbr.com/statistics/akaike-information-criterion/#:~:text=The%20AIC%20function%20is%202K,it%20is%20being%20compared%20to.

glmBase$aic
glmTop5$aic
glmTop3$aic

# Lower AIC values indicate a better-fit model, so we will choose glmTop5 because it has the lowest aic score, which is 12777.27.
# End Answer
```

## [5 points] Q32. 
*Knit to html after eliminating all the errors. Submit both the Rmd and html files.* 
*Tip: Do not worry about minor formatting issues.*
*Tip: This will take some time as you are processing medium size data sets.*

```{r}
### This section doesn't require code. Just knit and submit the Rmd and html files.### 
```


